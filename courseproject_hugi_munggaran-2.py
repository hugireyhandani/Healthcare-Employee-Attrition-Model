# -*- coding: utf-8 -*-
"""CourseProject_Hugi_Munggaran.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYNGNehkHd4ZBUTie0s9vQQORqH_QduL

# ***Data Importing***
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

data=pd.read_csv('watson_healthcare_modified.csv')
data.head()

# Using the 'shape' attribute, which returns a tuple with (number_of_rows, number_of_columns)
num_columns = data.shape[1]

# Using the 'columns' attribute, which returns an Index object with column labels
num_columns = len(data.columns)

# Print the number of columns
print(f'The dataset has {num_columns} columns.')

# Summary statistics
summary_statistics = data.describe()
print(summary_statistics)

"""# ***Data Cleaning and Preprocessing***"""

# Check for missing values
missing_values = data.isnull().sum()
print("Missing Values:\n", missing_values)

# Check for duplicate entries
duplicates = data.duplicated().sum()
print("\nNumber of Duplicate Rows:", duplicates)

# Converting Gender and OverTime to binary
data['Gender'] = data['Gender'].map({'Female': 1, 'Male': 0})
data['OverTime'] = data['OverTime'].map({'Yes': 1, 'No': 0})

# One-hot encoding for JobRole and MaritalStatus
data = pd.get_dummies(data, columns=['JobRole', 'MaritalStatus'])

# Display the first few rows of the updated dataset
print(data.head())

# Removing unneccessary column
data = data.drop('EmployeeID', axis=1)
data= data.drop('Over18', axis=1)

# Identifying non-numeric columns, excluding 'Attrition'
non_numeric_columns = data.select_dtypes(include=['object']).columns
non_numeric_columns = non_numeric_columns[non_numeric_columns != 'Attrition']

# Applying one-hot encoding to the non-numeric columns, excluding 'Attrition'
data = pd.get_dummies(data, columns=non_numeric_columns)

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Separating the target variable 'Attrition'
attrition = data['Attrition']
data_without_attrition = data.drop('Attrition', axis=1)

# Initializing the StandardScaler
scaler = StandardScaler()

# Scale the data without the 'Attrition' column
scaled_data_without_attrition = scaler.fit_transform(data_without_attrition)

# Converting the scaled data back to a DataFrame
scaled_data_without_attrition = pd.DataFrame(scaled_data_without_attrition, columns=data_without_attrition.columns)

# Concatenating the unscaled 'Attrition' column back
scaled_data = pd.concat([scaled_data_without_attrition, attrition.reset_index(drop=True)], axis=1)
scaled_data

# Converting the scaled data back to a DataFrame
scaled_data_df = pd.DataFrame(scaled_data, columns=data_without_attrition.columns)

# Calculate the correlation matrix
correlation_matrix = scaled_data_df.corr()

# Plotting the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Assuming 'scaled_data' is your DataFrame and 'Attrition' is the target variable
# Convert 'Attrition' to binary (if it's not already)
label_encoder = LabelEncoder()
attrition_binary = label_encoder.fit_transform(scaled_data['Attrition'])

# Separating the features and the target variable
X = scaled_data.drop('Attrition', axis=1)
y = attrition_binary

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# ***Data Modeling***"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train, y_train)
lr_predictions = lr.predict(X_test)

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
logreg_predictions = logreg.predict(X_test)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
lda_predictions = lda.predict(X_test)

qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, y_train)
qda_predictions = qda.predict(X_test)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf_predictions = rf.predict(X_test)

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_predictions = dt.predict(X_test)

from sklearn.svm import SVC

svm = SVC()
svm.fit(X_train, y_train)
svm_predictions = svm.predict(X_test)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_predictions = knn.predict(X_test)

from sklearn.metrics import accuracy_score

# Assuming all other models are classification models and are already trained
models = [logreg, lda, qda, rf, dt, svm, knn]
predictions = [logreg_predictions, lda_predictions, qda_predictions, rf_predictions, dt_predictions, svm_predictions, knn_predictions]
model_names = ['Logistic Regression', 'LDA', 'QDA', 'Random Forest', 'Decision Tree', 'SVM', 'KNN']

accuracies = {name: accuracy_score(y_test, pred) for name, pred in zip(model_names, predictions)}

"""# ***Data Importing***"""

import matplotlib.pyplot as plt

plt.bar(range(len(accuracies)), list(accuracies.values()), align='center')
plt.xticks(range(len(accuracies)), list(accuracies.keys()), rotation=45)
plt.title('Model Accuracies')
plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.show()

from sklearn.metrics import confusion_matrix, accuracy_score
import pandas as pd

# Initialize an empty DataFrame to store the results
results = pd.DataFrame(columns=['Model', 'TNR', 'TPR', 'Accuracy'])

for model_name, model_predictions in zip(model_names, predictions):
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_test, model_predictions).ravel()

    # Calculate TNR, TPR, and Accuracy
    tnr = tn / (tn + fp)
    tpr = tp / (tp + fn)
    accuracy = accuracy_score(y_test, model_predictions)

    # Append the results to the DataFrame
    results = results.append({'Model': model_name, 'TNR': tnr, 'TPR': tpr, 'Accuracy': accuracy}, ignore_index=True)

# Display the results
results

pd.set_option('display.max_columns', None)
scaled_data

from sklearn.metrics import f1_score
f1_scores = {}
for model_name, model_predictions in zip(model_names, predictions):
    f1 = f1_score(y_test, model_predictions)
    f1_scores[model_name] = f1

# Convert the F1 scores to a DataFrame for display
f1_scores_df = pd.DataFrame(list(f1_scores.items()), columns=['Model', 'F1 Score'])
f1_scores_df